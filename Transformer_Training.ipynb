{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.Dataset import DatasetUtils, MidiTransformerDataset\n",
    "from models.Transformer import TransformerModel\n",
    "\n",
    "import PlotUtils as pu\n",
    "import MidiUtils as mu\n",
    "from data.Song import Song\n",
    "\n",
    "dataset_path = \"/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0\"\n",
    "workspace = \"/Users/andreas/Development/Midi-Conversion/PianoTranscription\"\n",
    "\n",
    "# Computing the total length of the dataset is expensive, so we cache it here\n",
    "TRAIN_SET_TOTAL_LEN_DISC_100_TRANS = 112624\n",
    "VAL_SET_TOTAL_LEN_DISC_100_TRANS = 13764\n",
    "TEST_SET_TOTAL_LEN_DISC_100_TRANS = 14183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size=1, time_discretization=100, shuffle=True, precomp=False):\n",
    "    # Only create train, validation, test splits if they don't exist (unnecessarily slow)\n",
    "    if not (os.path.exists(os.path.join(dataset_path, 'train.txt')) and os.path.exists(os.path.join(dataset_path, 'test.txt')) and os.path.exists(os.path.join(dataset_path, 'validation.txt'))):\n",
    "        DatasetUtils.create_dataset_files(dataset_path)\n",
    "\n",
    "    train_set = MidiTransformerDataset(dataset_path, 'train', time_discretization, total_length=None, precomputed_midi=precomp)\n",
    "    val_set = MidiTransformerDataset(dataset_path, 'validation', time_discretization, total_length=None, precomputed_midi=precomp)\n",
    "    test_set = MidiTransformerDataset(dataset_path, 'test', time_discretization, total_length=None, precomputed_midi=precomp)\n",
    "        \n",
    "    train_data = DataLoader(train_set, batch_size=batch_size)\n",
    "    val_data = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_data = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable), total=len(iterable), ncols=150, desc=desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "print(F\"Using device '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit Single Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'device': device,\n",
    "    'discretization': 100,\n",
    "\n",
    "    'learning_rate': 1e-4, \n",
    "    'epochs': 1, \n",
    "    'dropout': 0.0,\n",
    "    'batch_size': 10\n",
    "}\n",
    "\n",
    "loaders = get_data_loaders(params['batch_size'], params['discretization'], precomp=True)\n",
    "train_loader, val_loader, _ = loaders\n",
    "model = TransformerModel(output_depth=129, d_model=512, nhead=1, d_hid=512, nlayers=1, dropout=params['dropout'], params=params)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "for i in range(1):\n",
    "    audio, midi, mask = next(train_iter)\n",
    "\n",
    "audio = audio.to(device)\n",
    "midi = midi.to(device)\n",
    "mask = mask.to(device)\n",
    "\n",
    "print(audio.shape)\n",
    "print(midi[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file_set = MidiTransformerDataset(dataset_path, 'single_file', params['discretization'], total_length=None, precomputed_midi=False, no_file_lengths=True)\n",
    "single_file_loader = DataLoader(single_file_set, batch_size=10)\n",
    "audio, midi, mask = next(iter(single_file_loader))\n",
    "audio = audio.to(device)\n",
    "midi = midi.to(device)\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for i in range(500):\n",
    "    train_loss = model.training_step(audio, midi, mask, mask)\n",
    "    if i % 25 == 0:\n",
    "        print(f'it: {i} loss: {train_loss}')\n",
    "\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # combine midi batches into a single tensor\n",
    "    midi_plot = midi.reshape(-1, midi.shape[2])\n",
    "    # Threshold to ignore \n",
    "    pu.plot_tensor_as_image(midi_plot.T, figure_shape=(16, 4), threshold=0.5)\n",
    "    pred_midi = model.forward(audio, midi, mask, mask)\n",
    "    pred_midi = torch.sigmoid(pred_midi)\n",
    "\n",
    "    pred_midi_plot = pred_midi.reshape(-1, pred_midi.shape[2])\n",
    "    pu.plot_tensor_as_image(pred_midi_plot.T, figure_shape=(16, 4), threshold=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original midi\n",
    "Song.start_time_tensor_to_midi(midi_plot.squeeze(), 'output/original_midi.midi', 100, note_threshold=0.5)\n",
    "mu.play_midi('output/original_midi.midi', output_path='output/original_midi.wav')\n",
    "\n",
    "# Play the midi\n",
    "Song.start_time_tensor_to_midi(pred_midi_plot.squeeze(), 'output/predicted_midi.midi', 100, note_threshold=0.5)\n",
    "mu.play_midi('output/predicted_midi.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_index = 0\n",
    "# Predict single time step\n",
    "# Combine audio batches into a single tensor\n",
    "audio_sequence = audio[sequence_index, :, :].reshape(-1, audio.shape[2])\n",
    "# audio_sequence = audio[sequence_index, :, :]\n",
    "print(f\"Shape of audio sequence: {audio_sequence.shape}\")\n",
    "midi_prediction = model.predict2(audio_sequence, audio_sequence.shape[0], threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.plot_tensor_as_image(midi[sequence_index, :, :].reshape(-1, midi.shape[2]).T, figure_shape=(16, 4))\n",
    "pu.plot_tensor_as_image(midi_prediction.T, figure_shape=(16, 4), threshold=None)\n",
    "\n",
    "# Play the midi\n",
    "Song.start_time_tensor_to_midi(midi_prediction.squeeze(), 'output/predicted_midi.midi', 100)\n",
    "mu.play_midi('output/predicted_midi.midi')\n",
    "\n",
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, data_loaders, params, workspace, log_name, function):\n",
    "    assert 'epochs' in params, 'Number of epochs not specified in params (\\'epochs\\')'\n",
    "    assert 'device' in params, 'Device not specified in params (\\'device\\')'\n",
    "    assert 'batch_size' in params, 'Batch size not specified in params (\\'batch_size\\')'\n",
    "    assert 'learning_rate' in params, 'Learning rate not specified in params (\\'learning_rate\\')'\n",
    "\n",
    "    model.to(params['device'])\n",
    "\n",
    "    logger_path = os.path.join(workspace, 'logs', log_name)\n",
    "    num_of_runs = len(os.listdir(logger_path)) if os.path.exists(logger_path) else 0\n",
    "    logger = SummaryWriter(os.path.join(logger_path, f'run_{num_of_runs + 1}'))\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    train_loader, val_loader, test_loader = data_loaders\n",
    "    val_iter = iter(val_loader)\n",
    "    best_loss = float('inf')\n",
    "    train_group_length = 100\n",
    "    val_group_length = 10\n",
    "    val_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        # training_loop = train_loader\n",
    "        train_iter = iter(training_loop)\n",
    "        val_iteration = 0\n",
    "        train_iteration = 0\n",
    "\n",
    "        for iteration_group in range(0, len(training_loop), train_group_length):        \n",
    "            train_loss = 0\n",
    "\n",
    "            for train_group_index in range(min(train_group_length, len(training_loop) - iteration_group * train_group_length)):\n",
    "                _, batch = next(train_iter)\n",
    "                # batch = next(train_iter)\n",
    "                train_iteration += 1\n",
    "                # print(f\"train_iteration: {train_iteration}, train_group_index: {train_group_index}, batch_size: {len(batch)}\")\n",
    "\n",
    "                # Actual training\n",
    "                src, tgt, pad_mask = batch\n",
    "                # Shift target to right by one\n",
    "                tgt = tgt[:, 1:]\n",
    "                # Invert pad mask\n",
    "                tgt_pad_mask = pad_mask[:, 1:]\n",
    "                loss = model.training_step(src, tgt, pad_mask, tgt_pad_mask)\n",
    "                train_loss += loss\n",
    "\n",
    "                # if train_iteration % 10 == 0:\n",
    "                #     print(f'Training Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss / (train_iteration + 1)}')\n",
    "\n",
    "                # Progress indicator\n",
    "                if train_iteration % 10 == 0:\n",
    "                    training_loop.set_postfix(curr_train_loss=\"{:.8f}\".format(\n",
    "                        train_loss / (train_group_index + 1)), val_loss=\"{:.8f}\".format(val_loss), refresh=True)\n",
    "                    logger.add_scalar(f'{log_name}/train_loss', loss, epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "\n",
    "            # print(f'Training Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss / len(train_loader)}')\n",
    "\n",
    "            # Validation\n",
    "            val_loss = 0\n",
    "            for _ in range(val_group_length):\n",
    "                val_iteration += 1\n",
    "                batch = next(val_iter)\n",
    "                src, tgt, pad_mask = batch\n",
    "                loss = model.validation_step(src, tgt, pad_mask, pad_mask)\n",
    "                logger.add_scalar(f'{log_name}/val_loss', loss, epoch * len(val_loader) + val_iteration)\n",
    "                val_loss += loss\n",
    "\n",
    "            val_loss /= val_group_length\n",
    "                \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                path = os.path.join(workspace, 'out_models', log_name, f'best_model.pt')\n",
    "                # Create path if it doesn't exist\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                model.save_state(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'device': device, 'learning_rate': 5e-4, 'epochs': 1, 'batch_size': 10}\n",
    "\n",
    "model = TransformerModel(output_depth=129, d_model=512, nhead=1, d_hid=1024, nlayers=1, dropout=0.1, params=params)\n",
    "train_data, val_data, test_data = get_data_loaders(params['batch_size'], time_discretization=100, precomp=True)\n",
    "\n",
    "print(f\"Train data length: {len(train_data)}\")\n",
    "\n",
    "train_loop(model, (train_data, val_data, test_data), params, workspace, 'transformer', None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoTranscription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
