{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from mido import MidiFile, MidiTrack, Message\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "import timeit\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from data.Dataset import MidiDataset, DatasetUtils, MidiIterDataset, MidiTransformerDataset\n",
    "import MidiUtils as mu\n",
    "from data.Note import Note\n",
    "from data.Song import Song\n",
    "import PlotUtils\n",
    "from models.Transformer import TransformerModel\n",
    "from data.Dataloader import MidiDataloader\n",
    "\n",
    "from models.Transformer import MidiTransformer\n",
    "from torch import nn\n",
    "\n",
    "dataset_path = \"/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0\"\n",
    "workspace = \"/Users/andreas/Development/Midi-Conversion/PianoTranscription\"\n",
    "\n",
    "# Computing the total length of the dataset is expensive, so we cache it here\n",
    "TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100 = 57412301\n",
    "VAL_SET_TOTAL_LENGTH_DISCRETIZED_100 = 7009869\n",
    "TEST_SET_TOTAL_LENGTH_DISCRETIZED_100 = 7214840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = './output/Planet_Earth_II.mp3'\n",
    "audio_tensor, file_sample_rate = torchaudio.load(audio_file, normalize=True)\n",
    "\n",
    "samples = audio_tensor.shape[1]\n",
    "print(F'audio_tensor length: {audio_tensor.shape}, sample_rate: {file_sample_rate}')\n",
    "print(F\"file_length seconds: {samples/file_sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MidiTransformerDataset(dataset_path, 'train', 100, TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=True)\n",
    "data_iter = iter(dataset)\n",
    "length = 480\n",
    "total_length = 0\n",
    "iteration_length = 0\n",
    "# while length == 480:\n",
    "#     audio, midi = next(data_iter)\n",
    "#     length = len(audio)\n",
    "#     total_length += length\n",
    "#     iteration_length += length\n",
    "#     if iteration_length > 10_000_000:\n",
    "#         print(F'total_length: {total_length}')\n",
    "#         iteration_length = 0\n",
    "#     if length != 480:\n",
    "#         print(F'total length: {total_length} length: {length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot audio and midi next to each other\n",
    "audio_path = '/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.wav'\n",
    "midi_path = '/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.midi'\n",
    "\n",
    "audio_tensor, file_sample_rate = torchaudio.load(audio_path, normalize=True)\n",
    "midi = MidiFile(midi_path)\n",
    "\n",
    "audio_length_s = audio_tensor.shape[1] / file_sample_rate\n",
    "audio_length_ms = (audio_tensor.shape[1] % file_sample_rate) / file_sample_rate * 1000\n",
    "print(f\"Audio length samples: {audio_tensor.shape[1]}, sample rate: {file_sample_rate}, length in time: {int(audio_length_s)}s {audio_length_ms}ms\")\n",
    "\n",
    "midi_length = midi.length\n",
    "print(f\"Midi length: {midi_length}\")\n",
    "discretization = 100\n",
    "midi_tensor = DatasetUtils.transform_midi_file(midi_path, discretization).T\n",
    "\n",
    "x_lim_seconds = max(audio_tensor.shape[1]/file_sample_rate, midi_length)\n",
    "x_lim_midi = int(x_lim_seconds * discretization)\n",
    "x_lim_audio = int(x_lim_seconds * file_sample_rate)\n",
    "\n",
    "print(f'lim_x midi: {x_lim_midi}, audio: {x_lim_audio}')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "# fig = plt.figure(figsize=(400, 20))\n",
    "# Add a subplot for the plot\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "# Plot audio waveform exactly from start to end\n",
    "ax1.plot(audio_tensor[0].numpy())\n",
    "# Transform midi to midi tensor\n",
    "ax1.set_xlim(0, x_lim_audio)\n",
    "\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "ax2.set_xlim(0, x_lim_midi)\n",
    "# Plot midi notes exactly from start to end in same plot\n",
    "midi_tensor = midi_tensor.to('cpu')\n",
    "ax2.imshow(midi_tensor, aspect='auto')\n",
    "\n",
    "ax1.axis(\"off\")\n",
    "ax2.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_seq1 = torch.stack([next(data_iter)[0] for i in range(10)])\n",
    "audio_seq2 = torch.stack([next(data_iter)[0] for i in range(15)])\n",
    "\n",
    "padded_seq = torch.nn.utils.rnn.pad_sequence([audio_seq1, audio_seq2], batch_first=True)\n",
    "print(f\"padded_seq: {padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MidiTransformerDataset(dataset_path, 'train', 100, TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=True)\n",
    "model = TransformerModel(output_depth=128, d_model=512, nhead=1, d_hid=512, nlayers=1, dropout=0.1)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "for i, value in enumerate(dataloader):\n",
    "    audio, midi, mask = value\n",
    "    print(f\"audio: {audio.shape}, midi: {midi.shape}\")\n",
    "    output = model.forward(src=audio, tgt=midi, src_pad_mask=mask, tgt_pad_mask=mask)\n",
    "    print(f\"output: {output.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'mps'\n",
    "model = MidiTransformer(\n",
    "    num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoTranscription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
