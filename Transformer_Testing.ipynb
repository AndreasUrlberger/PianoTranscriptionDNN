{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from mido import MidiFile, MidiTrack, Message\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "import timeit\n",
    "\n",
    "from data.Dataset import MidiDataset, DatasetUtils, MidiIterDataset, MidiTransformerDataset\n",
    "import MidiUtils as mu\n",
    "from data.Note import Note\n",
    "from data.Song import Song\n",
    "import PlotUtils\n",
    "from models.Transformer import TransformerModel\n",
    "from data.Dataloader import MidiDataloader\n",
    "\n",
    "dataset_path = \"/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0\"\n",
    "workspace = \"/Users/andreas/Development/Midi-Conversion/PianoTranscription\"\n",
    "\n",
    "# Computing the total length of the dataset is expensive, so we cache it here\n",
    "TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100 = 57412301\n",
    "VAL_SET_TOTAL_LENGTH_DISCRETIZED_100 = 7009869\n",
    "TEST_SET_TOTAL_LENGTH_DISCRETIZED_100 = 7214840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = './output/Planet_Earth_II.mp3'\n",
    "audio_tensor, file_sample_rate = torchaudio.load(audio_file, normalize=True)\n",
    "\n",
    "samples = audio_tensor.shape[1]\n",
    "print(F'audio_tensor length: {audio_tensor.shape}, sample_rate: {file_sample_rate}')\n",
    "print(F\"file_length seconds: {samples/file_sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MidiTransformerDataset(dataset_path, 'train', 100, TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=True)\n",
    "data_iter = iter(dataset)\n",
    "length = 480\n",
    "total_length = 0\n",
    "iteration_length = 0\n",
    "# while length == 480:\n",
    "#     audio, midi = next(data_iter)\n",
    "#     length = len(audio)\n",
    "#     total_length += length\n",
    "#     iteration_length += length\n",
    "#     if iteration_length > 10_000_000:\n",
    "#         print(F'total_length: {total_length}')\n",
    "#         iteration_length = 0\n",
    "#     if length != 480:\n",
    "#         print(F'total length: {total_length} length: {length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot audio and midi next to each other\n",
    "audio_path = '/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.wav'\n",
    "midi_path = '/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0/2018/MIDI-Unprocessed_Chamber2_MID--AUDIO_09_R3_2018_wav--1.midi'\n",
    "\n",
    "audio_tensor, file_sample_rate = torchaudio.load(audio_path, normalize=True)\n",
    "midi = MidiFile(midi_path)\n",
    "\n",
    "audio_length_s = audio_tensor.shape[1] / file_sample_rate\n",
    "audio_length_ms = (audio_tensor.shape[1] % file_sample_rate) / file_sample_rate * 1000\n",
    "print(f\"Audio length samples: {audio_tensor.shape[1]}, sample rate: {file_sample_rate}, length in time: {int(audio_length_s)}s {audio_length_ms}ms\")\n",
    "\n",
    "midi_length = midi.length\n",
    "print(f\"Midi length: {midi_length}\")\n",
    "discretization = 100\n",
    "midi_tensor = DatasetUtils.transform_midi_file(midi_path, discretization).T\n",
    "\n",
    "x_lim_seconds = max(audio_tensor.shape[1]/file_sample_rate, midi_length)\n",
    "x_lim_midi = int(x_lim_seconds * discretization)\n",
    "x_lim_audio = int(x_lim_seconds * file_sample_rate)\n",
    "\n",
    "print(f'lim_x midi: {x_lim_midi}, audio: {x_lim_audio}')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "# fig = plt.figure(figsize=(400, 20))\n",
    "# Add a subplot for the plot\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "# Plot audio waveform exactly from start to end\n",
    "ax1.plot(audio_tensor[0].numpy())\n",
    "# Transform midi to midi tensor\n",
    "ax1.set_xlim(0, x_lim_audio)\n",
    "\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "ax2.set_xlim(0, x_lim_midi)\n",
    "# Plot midi notes exactly from start to end in same plot\n",
    "midi_tensor = midi_tensor.to('cpu')\n",
    "ax2.imshow(midi_tensor, aspect='auto')\n",
    "\n",
    "ax1.axis(\"off\")\n",
    "ax2.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_seq1 = torch.stack([next(data_iter)[0] for i in range(10)])\n",
    "audio_seq2 = torch.stack([next(data_iter)[0] for i in range(15)])\n",
    "\n",
    "padded_seq = torch.nn.utils.rnn.pad_sequence([audio_seq1, audio_seq2], batch_first=True)\n",
    "print(f\"padded_seq: {padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "elems = 30\n",
    "for i in range(0, elems - seq_len + 1, seq_len):\n",
    "    print(f\"i: {i}\")\n",
    "\n",
    "print(f\"last sequence start index: {elems // seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MidiTransformerDataset(dataset_path, 'train', 100, TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=True)\n",
    "model = TransformerModel(ntoken=480, d_model=512, nhead=1, d_hid=512, nlayers=1, dropout=0.1)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "for i, value in enumerate(dataloader):\n",
    "    audio, midi, mask = value\n",
    "    print(f\"audio: {audio.shape}, midi: {midi.shape}\")\n",
    "    output = model.forward(audio, mask)\n",
    "    print(f\"output: {output.shape}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoTranscription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
