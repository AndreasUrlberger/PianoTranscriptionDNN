{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-Midi Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "import MidiUtils as mu\n",
    "import PlotUtils as pu\n",
    "from Dataset import MidiDataset\n",
    "from Model import MidiTranscriptionModel\n",
    "from Note import Note\n",
    "from Song import Song\n",
    "\n",
    "dataset_path = \"/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0\"\n",
    "workspace = \"/Users/andreas/Development/Midi-Conversion/PianoTranscription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size=1, time_discretization=100, train_set_size=None, val_set_size=None, shuffle=True):\n",
    "    # Only create train, validation, test splits if they don't exist (unnecessarily slow)\n",
    "    if not (os.path.exists(os.path.join(dataset_path, 'train.txt')) and os.path.exists(os.path.join(dataset_path, 'test.txt')) and os.path.exists(os.path.join(dataset_path, 'validation.txt'))):\n",
    "        MidiDataset.create_dataset_files(dataset_path)\n",
    "\n",
    "    train_set = MidiDataset(dataset_path, 'train', time_discretization)\n",
    "    val_set = MidiDataset(dataset_path, 'validation', time_discretization)\n",
    "    test_set = MidiDataset(dataset_path, 'test', time_discretization)\n",
    "\n",
    "    if train_set_size is not None:\n",
    "        limit_type, limit = train_set_size\n",
    "        if limit_type == 'items':  # At least 1 item, at most all items\n",
    "            limit = max(min(limit, len(train_set)), 1)\n",
    "        elif limit_type == 'percentage':  # At least 1 item, at most all items\n",
    "            limit = max(min(limit * len(train_set), len(train_set)), 1)\n",
    "        else:\n",
    "            limit = len(train_set)\n",
    "        train_set = data_utils.Subset(train_set, torch.arange(limit))\n",
    "\n",
    "    if val_set_size is not None:\n",
    "        limit_type, limit = val_set_size\n",
    "        if limit_type == 'items':  # At least 1 item, at most all items\n",
    "            limit = max(min(limit, len(val_set)), 1)\n",
    "        elif limit_type == 'percentage':  # At least 1 item, at most all items\n",
    "            limit = max(min(limit * len(val_set), len(val_set)), 1)\n",
    "        else:\n",
    "            limit = len(val_set)\n",
    "        val_set = data_utils.Subset(val_set, torch.arange(limit))\n",
    "\n",
    "    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_data = DataLoader(val_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_data = DataLoader(test_set, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable), total=len(iterable), ncols=150, desc=desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, data_loaders, params, workspace, log_name, function):\n",
    "    assert 'epochs' in params, 'Number of epochs not specified in params (\\'epochs\\')'\n",
    "    assert 'device' in params, 'Device not specified in params (\\'device\\')'\n",
    "    assert 'batch_size' in params, 'Batch size not specified in params (\\'batch_size\\')'\n",
    "    assert 'learning_rate' in params, 'Learning rate not specified in params (\\'learning_rate\\')'\n",
    "\n",
    "    model.to(params['device'])\n",
    "\n",
    "    logger_path = os.path.join(workspace, 'logs', log_name)\n",
    "    num_of_runs = len(os.listdir(logger_path)) if os.path.exists(logger_path) else 0\n",
    "    logger = SummaryWriter(os.path.join(logger_path, f'run_{num_of_runs + 1}'))\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    train_loader, val_loader, test_loader = data_loaders\n",
    "    last_time_ms = time.time()/10e6\n",
    "    val_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(\n",
    "            train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        train_loss = 0\n",
    "        for train_iteration, batch in training_loop:\n",
    "            # Actual training\n",
    "            loss = model.training_step(batch)\n",
    "            train_loss += loss\n",
    "\n",
    "            # Progress indicator\n",
    "            if time.time_ns()/10e6 - last_time_ms > 10:\n",
    "                training_loop.set_postfix(curr_train_loss=\"{:.8f}\".format(\n",
    "                    train_loss / (train_iteration + 1)), val_loss=\"{:.8f}\".format(val_loss), refresh=True)\n",
    "                last_time_ms = time.time_ns()/10e6\n",
    "                logger.add_scalar(f'{log_name}/train_loss', loss, epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        print(f'Training Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss / len(train_loader)}')\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        for val_iteration, batch in enumerate(val_loader):\n",
    "            # Actual validation\n",
    "            loss = model.validation_step(batch)\n",
    "            val_loss += loss\n",
    "            logger.add_scalar(f'{log_name}/val_loss', loss,\n",
    "                              epoch * len(val_loader) + val_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'device': 'cpu',\n",
    "    'batch_size': 1,\n",
    "    'train_set_size': ('items', 1), # ('percent', 0.1) = 10 percent, None = all\n",
    "    'val_set_size': ('items', 1), # ('percent', 0.1) = 10 percent, None = all\n",
    "    'discretization': 100,\n",
    "\n",
    "    'learning_rate': 3e-2,\n",
    "    'epochs': 1,\n",
    "\n",
    "    'input_size': 480,\n",
    "    'output_size': 128,\n",
    "}\n",
    "\n",
    "loaders = get_data_loaders(params['batch_size'], params['discretization'], train_set_size=params['train_set_size'], val_set_size=params['val_set_size'], shuffle=False)\n",
    "model = MidiTranscriptionModel(params=params)\n",
    "\n",
    "train_loop(function=None, model=model, data_loaders=loaders, params=params, workspace=workspace, log_name='overfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a song\n",
    "train_loader, _, _ = loaders\n",
    "audio, midi = train_loader.dataset[0]\n",
    "pred_midi = model.predict(audio)\n",
    "pu.plot_tensor_as_image(pred_midi.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song.start_time_tensor_to_midi(pred_midi, 'predicted_midi.midi', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test back and forth conversion of simple midi\n",
    "midi = MidiDataset.transform_midi_file('output.mid', 100)\n",
    "pu.plot_tensor_as_image(midi)\n",
    "Song.start_time_tensor_to_midi(midi, 'converted_output.mid', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoTranscription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
