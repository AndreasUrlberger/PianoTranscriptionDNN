{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-Midi Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "import MidiUtils as mu\n",
    "import PlotUtils as pu\n",
    "from data.Dataset import MidiDataset, DatasetUtils, MidiIterDataset\n",
    "from models.DNNs import MidiTranscriptionModel\n",
    "from data.Note import Note\n",
    "from data.Song import Song\n",
    "\n",
    "dataset_path = \"/Users/andreas/Development/Midi-Conversion/maestro-v3.0.0\"\n",
    "workspace = \"/Users/andreas/Development/Midi-Conversion/PianoTranscription\"\n",
    "\n",
    "# Computing the total length of the dataset is expensive, so we cache it here\n",
    "TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100 = 57412301\n",
    "VAL_SET_TOTAL_LENGTH_DISCRETIZED_100 = 7009869\n",
    "TEST_SET_TOTAL_LENGTH_DISCRETIZED_100 = 7214840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size=1, time_discretization=100, train_set_size=None, val_set_size=None, shuffle=True, iter_dataset=False, precomp=False):\n",
    "    # Only create train, validation, test splits if they don't exist (unnecessarily slow)\n",
    "    if not (os.path.exists(os.path.join(dataset_path, 'train.txt')) and os.path.exists(os.path.join(dataset_path, 'test.txt')) and os.path.exists(os.path.join(dataset_path, 'validation.txt'))):\n",
    "        DatasetUtils.create_dataset_files(dataset_path)\n",
    "\n",
    "    if iter_dataset:\n",
    "        train_set = MidiIterDataset(dataset_path, 'train', time_discretization, total_length=TRAIN_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=precomp)\n",
    "        val_set = MidiIterDataset(dataset_path, 'validation', time_discretization, total_length=VAL_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=precomp)\n",
    "        test_set = MidiIterDataset(dataset_path, 'test', time_discretization, total_length=TEST_SET_TOTAL_LENGTH_DISCRETIZED_100, precomputed_midi=precomp)\n",
    "    else:\n",
    "        train_set = MidiDataset(dataset_path, 'train', time_discretization)\n",
    "        val_set = MidiDataset(dataset_path, 'validation', time_discretization)\n",
    "        test_set = MidiDataset(dataset_path, 'test', time_discretization)\n",
    "\n",
    "        if train_set_size is not None:\n",
    "            limit_type, limit = train_set_size\n",
    "            if limit_type == 'items':  # At least 1 item, at most all items\n",
    "                limit = max(min(limit, len(train_set)), 1)\n",
    "            elif limit_type == 'percentage':  # At least 1 item, at most all items\n",
    "                limit = max(min(limit * len(train_set), len(train_set)), 1)\n",
    "            else:\n",
    "                limit = len(train_set)\n",
    "            train_set = data_utils.Subset(train_set, torch.arange(limit))\n",
    "\n",
    "        if val_set_size is not None:\n",
    "            limit_type, limit = val_set_size\n",
    "            if limit_type == 'items':  # At least 1 item, at most all items\n",
    "                limit = max(min(limit, len(val_set)), 1)\n",
    "            elif limit_type == 'percentage':  # At least 1 item, at most all items\n",
    "                limit = max(min(limit * len(val_set), len(val_set)), 1)\n",
    "            else:\n",
    "                limit = len(val_set)\n",
    "            val_set = data_utils.Subset(val_set, torch.arange(limit))\n",
    "\n",
    "    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_data = DataLoader(val_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_data = DataLoader(test_set, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable), total=len(iterable), ncols=150, desc=desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: torch.nn.Module, data_loaders, params, workspace, log_name, function):\n",
    "    assert 'epochs' in params, 'Number of epochs not specified in params (\\'epochs\\')'\n",
    "    assert 'device' in params, 'Device not specified in params (\\'device\\')'\n",
    "    assert 'batch_size' in params, 'Batch size not specified in params (\\'batch_size\\')'\n",
    "    assert 'learning_rate' in params, 'Learning rate not specified in params (\\'learning_rate\\')'\n",
    "\n",
    "    model.to(params['device'])\n",
    "\n",
    "    logger_path = os.path.join(workspace, 'logs', log_name)\n",
    "    num_of_runs = len(os.listdir(logger_path)) if os.path.exists(logger_path) else 0\n",
    "    logger = SummaryWriter(os.path.join(logger_path, f'run_{num_of_runs + 1}'))\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    train_loader, val_loader, test_loader = data_loaders\n",
    "    last_time_ms = time.time()/10e6\n",
    "    val_loss = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        train_loss = 0\n",
    "        for train_iteration, batch in training_loop:\n",
    "\n",
    "            if train_iteration % 1000 == 0:\n",
    "                print(f'Training Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss / (train_iteration + 1)}')\n",
    "            # Actual training\n",
    "            loss = model.training_step(batch)\n",
    "            train_loss += loss\n",
    "\n",
    "            # Progress indicator\n",
    "            if time.time_ns()/10e6 - last_time_ms > 10:\n",
    "                training_loop.set_postfix(curr_train_loss=\"{:.8f}\".format(\n",
    "                    train_loss / (train_iteration + 1)), val_loss=\"{:.8f}\".format(val_loss), refresh=True)\n",
    "                last_time_ms = time.time_ns()/10e6\n",
    "                logger.add_scalar(f'{log_name}/train_loss', loss, epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        print(f'Training Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss / len(train_loader)}')\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        for val_iteration, batch in enumerate(val_loader):\n",
    "            # Actual validation\n",
    "            loss = model.validation_step(batch)\n",
    "            val_loss += loss\n",
    "            logger.add_scalar(f'{log_name}/val_loss', loss,\n",
    "                              epoch * len(val_loader) + val_iteration)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            path = os.path.join(workspace, 'out_models', log_name, f'best_model.pt')\n",
    "            # Create path if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            model.save_state(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'device': device,\n",
    "    'batch_size': 1,\n",
    "    'train_set_size': ('items', 1), # ('percent', 0.1) = 10 percent, None = all\n",
    "    'val_set_size': ('items', 1), # ('percent', 0.1) = 10 percent, None = all\n",
    "    'discretization': 100,\n",
    "\n",
    "    'input_size': 480,\n",
    "    'hidden_size_1': 720,\n",
    "    'hidden_size_2': 1024,\n",
    "    'hidden_size_3': 720,\n",
    "    'hidden_size_4': 256,\n",
    "    'output_size': 128,\n",
    "\n",
    "    'learning_rate': 2e-3,\n",
    "    'epochs': 3,\n",
    "}\n",
    "\n",
    "loaders = get_data_loaders(params['batch_size'], params['discretization'], train_set_size=params['train_set_size'], val_set_size=params['val_set_size'], shuffle=False)\n",
    "model = MidiTranscriptionModel(params=params)\n",
    "\n",
    "train_loop(function=None, model=model, data_loaders=loaders, params=params, workspace=workspace, log_name='overfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a song\n",
    "train_loader, _, _ = loaders\n",
    "audio, midi = train_loader.dataset[0]\n",
    "audio = audio.to(device)\n",
    "midi = midi.to(device)\n",
    "pred_midi = model.predict(audio)\n",
    "pu.plot_tensor_as_image(pred_midi.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song.start_time_tensor_to_midi(pred_midi, 'output/predicted_midi.midi', 100, note_threshold=0.1)\n",
    "mu.play_midi('output/predicted_midi.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song.start_time_tensor_to_midi(midi, 'output/reconverted.midi', 100, note_threshold=0.5)\n",
    "mu.play_midi('output/reconverted.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_iter(model: torch.nn.Module, data_loaders, params, workspace, log_name):\n",
    "    assert 'epochs' in params, 'Number of epochs not specified in params (\\'epochs\\')'\n",
    "    assert 'device' in params, 'Device not specified in params (\\'device\\')'\n",
    "    assert 'batch_size' in params, 'Batch size not specified in params (\\'batch_size\\')'\n",
    "    assert 'learning_rate' in params, 'Learning rate not specified in params (\\'learning_rate\\')'\n",
    "\n",
    "    model.to(params['device'])\n",
    "\n",
    "    logger_path = os.path.join(workspace, 'logs', log_name)\n",
    "    num_of_runs = len(os.listdir(logger_path)) if os.path.exists(logger_path) else 0\n",
    "    logger = SummaryWriter(os.path.join(logger_path, f'run_{num_of_runs + 1}'))\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    train_loader, val_loader, _ = data_loaders\n",
    "    last_time_ms = time.time()/10e6\n",
    "    last_time_val_ms = time.time()/10e6\n",
    "    val_loss = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        train_loss = 0\n",
    "        val_iteration = 0\n",
    "        for train_iteration, batch in training_loop:\n",
    "            loss = model.training_step(batch)\n",
    "            train_loss += loss\n",
    "\n",
    "            # Progress indicator\n",
    "            if time.time_ns()/10e6 - last_time_ms > 20:\n",
    "                training_loop.set_postfix(curr_train_loss=\"{:.8f}\".format(\n",
    "                    train_loss / (train_iteration + 1)), val_loss=\"{:.8f}\".format(val_loss), refresh=True)\n",
    "                last_time_ms = time.time_ns()/10e6\n",
    "                logger.add_scalar(f'{log_name}/train_loss', loss, epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "            if time.time_ns()/10e6 - last_time_val_ms > 1000:\n",
    "                last_time_val_ms = time.time_ns()/10e6\n",
    "                # Validation\n",
    "                val_time_start = time.time_ns()/10e6\n",
    "                val_loss = 0\n",
    "                while (time.time_ns()/10e6 - val_time_start) < 200:\n",
    "                    # Get next batch from validation loader\n",
    "                    batch = next(iter(val_loader))\n",
    "\n",
    "                    # Actual validation\n",
    "                    loss = model.validation_step(batch)\n",
    "                    val_loss += loss\n",
    "                    logger.add_scalar(f'{log_name}/val_loss', loss,\n",
    "                                    epoch * len(val_loader) + val_iteration)\n",
    "                    val_iteration += 1\n",
    "\n",
    "                # if val_loss < best_loss:\n",
    "                #     best_loss = val_loss\n",
    "                #     path = os.path.join(workspace, 'out_models', log_name, f'best_model.pt')\n",
    "                #     # Create path if it doesn't exist\n",
    "                #     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                #     model.save_state(path)\n",
    "\n",
    "            \n",
    "\n",
    "        print(f'Training Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss / len(train_loader)}')\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'device': device,\n",
    "    'batch_size': 4096*2,\n",
    "    'train_set_size': None, # ('percent', 0.1) = 10 percent, None = all\n",
    "    'val_set_size': None, # ('items', 1) = 1 elem, ('percent', 0.1) = 10 percent, None = all\n",
    "    'discretization': 100,\n",
    "\n",
    "    'input_size': 480,\n",
    "    'hidden_size_1': 1024,\n",
    "    'hidden_size_2': 2048,\n",
    "    'hidden_size_3': 1440,\n",
    "    'hidden_size_4': 512,\n",
    "    'output_size': 128,\n",
    "\n",
    "    'learning_rate': 3e-3,\n",
    "    'epochs': 1,\n",
    "}\n",
    "\n",
    "loaders = get_data_loaders(params['batch_size'], params['discretization'], train_set_size=params['train_set_size'], val_set_size=params['val_set_size'], shuffle=False, iter_dataset=True, precomp=True)\n",
    "model = MidiTranscriptionModel(params=params)\n",
    "\n",
    "train_loop_iter(model=model, data_loaders=loaders, params=params, workspace=workspace, log_name='iter_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, _, _ = loaders\n",
    "# Get next element from iterable dataset\n",
    "audio, midi = next(iter(train_loader))\n",
    "audio = audio.to(device)\n",
    "# Predict the midi\n",
    "pred_midi = model.predict(audio)\n",
    "pu.plot_tensor_as_image(pred_midi.T)\n",
    "# Play the midi\n",
    "Song.start_time_tensor_to_midi(pred_midi, 'output/predicted_midi.midi', 100, note_threshold=0.1)\n",
    "mu.play_midi('output/predicted_midi.midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test overfitting single chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'device': device,\n",
    "    'batch_size': 20000,\n",
    "    'train_set_size': None, # ('percent', 0.1) = 10 percent, None = all\n",
    "    'val_set_size': None, # ('items', 1) = 1 elem, ('percent', 0.1) = 10 percent, None = all\n",
    "    'discretization': 100,\n",
    "\n",
    "    'input_size': 480,\n",
    "    'hidden_size_1': 1024,\n",
    "    'hidden_size_2': 2048,\n",
    "    'hidden_size_3': 1440,\n",
    "    'hidden_size_4': 512,\n",
    "    'output_size': 128,\n",
    "\n",
    "    'learning_rate': 6e-3,\n",
    "    'epochs': 1,\n",
    "}\n",
    "\n",
    "loaders = get_data_loaders(params['batch_size'], params['discretization'], train_set_size=params['train_set_size'], val_set_size=params['val_set_size'], shuffle=False, iter_dataset=True, precomp=True)\n",
    "train_loader, val_loader, _ = loaders\n",
    "model = MidiTranscriptionModel(params=params)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "for i in range(1):\n",
    "    audio, midi = next(train_iter)\n",
    "\n",
    "audio = audio.to(device)\n",
    "midi = midi.to(device)\n",
    "\n",
    "print(audio.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "for i in range(500):\n",
    "    train_loss = model.training_step((audio, midi))\n",
    "    if i % 25 == 0:\n",
    "        print(f'it: {i} loss: {train_loss}')\n",
    "\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.plot_tensor_as_image(midi.T, figure_shape=(16, 4))\n",
    "pred_midi = model.predict(audio)\n",
    "pu.plot_tensor_as_image(pred_midi.T, figure_shape=(16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the midi\n",
    "Song.start_time_tensor_to_midi(pred_midi, 'output/predicted_midi.midi', 100, note_threshold=0.5)\n",
    "mu.play_midi('output/predicted_midi.midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoTranscription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
